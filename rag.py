# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11vcNqYJhfQKuRT_YU5mQMuB8yzRJgJ7j
"""

# Commented out IPython magic to ensure Python compatibility.
#move to the folder where the scraped data is on your device
#from google.colab import drive
#drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Data/WiChat/scraped_data

#!pip install virtualenv
#!virtualenv /content/drive/MyDrive/Data/WiChat
#!source /content/drive/MyDrive/Data/WiChat/bin/activate

#!curl -fsSL https://ollama.com/install.sh | sh

# Commented out IPython magic to ensure Python compatibility.
#!pip install colab-xterm
# %load_ext colabxterm

# run each code separately in the terminal to connect to ollama server
#ollama serve &
#ollama pull phi3:mini

# Commented out IPython magic to ensure Python compatibility.
# %xterm

import os
os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'


from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter


# ensure the notebook is in the same folder as the data files
# load all txt files
def load_data(path):
  loader1 = DirectoryLoader(path, glob = '*.txt', show_progress = True)
  # get content of txt files
  docs = loader1.load()

  return docs


def get_chunks(docs):

    # split the txt files into chunks of 1000 characters and 150 characters overlap
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 150)
    chunks = text_splitter.split_documents(docs)


    return chunks

path = '/content/drive/MyDrive/Data/WiChat/scraped_data'
docs = load_data(path)

data = get_chunks(docs)

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# embed data sources
def embed(data, device, model):
  model_kwargs = {'device': device}
  encode_kwargs = {'normalize_embeddings': False}

  embeddings = HuggingFaceEmbeddings(
    model_name = model,
    model_kwargs = model_kwargs,
    encode_kwargs = encode_kwargs
  )
  return embeddings

def store_data(data, embeddings):
  # vector store
  db = FAISS.from_documents(data, embeddings)
  return db

embeddings = embed(data, 'cpu', 'sentence-transformers/all-MiniLM-L6-v2')
db = store_data(data, embeddings)

# import libraries
from langchain_ollama import ChatOllama
from langchain.chains.llm import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain,RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

def generate_response(db,question):
    model = ChatOllama(model="phi3:mini", temperature=0)

    # perform a similarity search and retrieve the context from our documents
    results = db.similarity_search(question, k=3)
        # join all context information into one string
    context = "n".join([document.page_content for document in results])

         # chatbot initial prompt template
    template = """You are WiChat, the chatbot for the Worldbank Ideas Project. You are friendly and follow instructions to answer questions extremely well. Please be truthful and give direct answers. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the response short and concise in at most five sentences. If the user chats in a different language, translate accurately and respond in the same language. You will provide specific details and accurate answers to user queries on the Worldbank Ideas Project.
                   Use the following pieces of context to answer the user's question.
                   Context: {context}
                   Question:{question}
                   Helpful Answer: """
    prompt = PromptTemplate(input_variables=["context",  "question"], template=template)

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


    retrievalqa = RetrievalQA.from_chain_type(
        llm=model,
        chain_type="stuff",
        retriever=db.as_retriever(search_type="similarity", search_kwargs={"k": 3}),
        memory = memory,
        chain_type_kwargs={"prompt": prompt}
    )
    answer = retrievalqa({"query": question})
    return answer["result"]

query = "What is Worldbank Ideas Project?"
generate_response(db,query)

query = "How can i be a part of this project"
generate_response(db,query)

query = "What role does Uniccon play in this project?"
generate_response(db,query)

query = "Quel est ton nom?"
generate_response(db,query)
